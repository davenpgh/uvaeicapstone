{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aaf1ce7-831f-4363-a502-f8a95e71b9a3",
   "metadata": {},
   "source": [
    "# Deep learning Project (continued)\n",
    "\n",
    "## Predicting County Suitability for Various CO2 Removal Technologies in Virginia\n",
    "\n",
    "**Motivation:**\n",
    "\n",
    "Carbon removal is a broad set of approaches, some natural and some engineered, used to remove CO2 from the atmosphere. The approaches can be as simple as reforestation, or as complex as growing bioenergy crops for producing bioenergy with carbon capture. We will develop a various Neural Networks to predict how suitable land is for various CO2 removal techniques in VA at the county scale. This will help researchers with the goal of reducing carbon emissions by providing information on how to utilize available and suitable land for reforestation plans,  enhanced weathering (EW), and biochar.\n",
    "\n",
    "\n",
    "**Technical plan:**\n",
    "\n",
    "We will input GGR technique specific predictor variables into an unsupervised algorithm, KMeans, to assign a ‘suitability level’ to each county in VA. Next, these labeled counties will be fed into a feed-forward neural network, a simple ANN, and a deep and cross neutal network. The neural nets will output the ‘suitability level’ for implementing the CO2 removal techniques of reforestation, EW, and biochar respectively. We will create our own models and utilize pre-trained moels like the MLPClassifier from sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e76ba-8218-441c-a9ff-733fdeccfbd8",
   "metadata": {},
   "source": [
    "# Code for Hierarchical Clustering to obtain labels (suitability category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d0a329-76be-45d7-95a9-79ae034afc88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-17 16:11:01.935986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 16:11:02.521066: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-17 16:11:06.588039: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib:/opt/slurm/current/lib:/share/rci_apps/common/lib64\n",
      "2024-04-17 16:11:06.608394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib:/opt/slurm/current/lib:/share/rci_apps/common/lib64\n",
      "2024-04-17 16:11:06.608412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# To get model performance \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "# loading everything that might be helpful\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import Generator\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import manual_seed as torch_manual_seed\n",
    "from torch.cuda import max_memory_allocated, set_device, manual_seed_all\n",
    "from torch.backends import cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from deeptables.models import deeptable, deepnets\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Set seed for whole notebook\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d62136-a1fa-4a22-87f9-2ada24b54850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading locational dataset\n",
    "df = pd.read_csv('../final-dataset/final-scaled-df.csv', index_col='FIPS')\n",
    "df.head(20)\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "size_all = len(df)\n",
    "\n",
    "# training/val/test dataset\n",
    "size_train = size_all - 6\n",
    "size_val = int(size_train * TEST_RATIO)\n",
    "size_train_sub = size_train - size_val\n",
    "\n",
    "# split data into train and test\n",
    "# Test = 6 counties of interst: Accomack, Fauquier, Greensville, Hanover, Rockingham, Wise\n",
    "dataset_val = df.loc[[51001, 51061, 51081, 51085, 51165, 51195]]\n",
    "\n",
    "# Train = all except 6 counties of interest\n",
    "dataset_train = df.drop([51001, 51061, 51081, 51085, 51165, 51195])\n",
    "\n",
    "cols = ['Income', 'DSCI', 'PQ1', 'PQ2', 'PQ3', 'PQ4', 'TQ1', 'TQ2', 'TQ3', 'TQ4', 'Forest', \n",
    "        'Agriculture', 'Biomass', 'Power', 'Water Availability', 'Watershed',\n",
    "       'Log_Bio','Log_Water','Log_Power']\n",
    "cols_rf = ['PQ1', 'PQ2', 'PQ3', 'PQ4', 'TQ1', 'TQ2', 'TQ3', 'TQ4', 'Forest','Water Availability', 'Log_Water']\n",
    "cols_ew = ['PQ1', 'PQ2', 'PQ3', 'PQ4', 'TQ1', 'TQ2', 'TQ3', 'TQ4', 'Income', 'Power', 'Log_Power', 'Agriculture']\n",
    "cols_bio = ['PQ1', 'PQ2', 'PQ3', 'PQ4', 'TQ1', 'TQ2', 'TQ3', 'TQ4', 'Income', 'Biomass', 'Log_Bio','Agriculture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1880c17e-1727-41b3-8686-48f36dc77786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the elbow method to determine the k value to be applied\n",
    "k_rng = range(1, 10)\n",
    "sse = []\n",
    "for k in k_rng:\n",
    "    km = KMeans(n_clusters=k, n_init=10)\n",
    "    km.fit(df[['PQ1', 'PQ2', 'PQ3', 'PQ4', 'TQ1', 'TQ2', 'TQ3', 'TQ4', 'Forest','Log_Water']])\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# create clusters using k value = 4\n",
    "hc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n",
    "\n",
    "# Determining mean cluster characterisitics REFORESTATION\n",
    "y_hc = hc.fit_predict(dataset_train[cols_rf])\n",
    "dataset_train['Reforest'] = y_hc\n",
    "dataset_train.sort_values(\"Reforest\", inplace = True, ascending=True)\n",
    "\n",
    "#average input vars by cluster\n",
    "df_rfcluster = dataset_train.groupby('Reforest').mean()\n",
    "\n",
    "# Determining mean cluster characterisitics ENHANCED WEATHERING\n",
    "y_hc = hc.fit_predict(dataset_train[cols_ew])\n",
    "dataset_train['EW'] = y_hc\n",
    "dataset_train.sort_values(\"EW\", inplace = True, ascending=True)\n",
    "\n",
    "#average input vars by cluster\n",
    "df_ewcluster = dataset_train.groupby('EW').mean()\n",
    "\n",
    "# Determining mean cluster characterisitics BIOCHAR\n",
    "y_hc = hc.fit_predict(dataset_train[cols_bio])\n",
    "dataset_train['Biochar'] = y_hc\n",
    "dataset_train.sort_values(\"Biochar\", inplace = True, ascending=True)\n",
    "\n",
    "#average input vars by cluster\n",
    "df_biocluster = dataset_train.groupby('Biochar').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901ed72a-8b98-46c8-bc37-599ce8284c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating input identity\n",
    "sub_df = dataset_train[cols]\n",
    "X = sub_df\n",
    "y = dataset_train[['Reforest','EW','Biochar']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)\n",
    "\n",
    "X_train_rf = X_train[cols_rf]\n",
    "X_train_ew = X_train[cols_ew]\n",
    "X_train_bio = X_train[cols_bio]\n",
    "\n",
    "X_test_rf = X_test[cols_rf]\n",
    "X_test_ew = X_test[cols_ew]\n",
    "X_test_bio = X_test[cols_bio]\n",
    "\n",
    "y_train_rf = y_train[['Reforest']]\n",
    "y_train_ew = y_train[['EW']]\n",
    "y_train_bio = y_train[['Biochar']]\n",
    "\n",
    "y_test_rf = y_test[['Reforest']]\n",
    "y_test_ew = y_test[['EW']]\n",
    "y_test_bio = y_test[['Biochar']]\n",
    "\n",
    "dataset_val_rf = dataset_val[cols_rf]\n",
    "dataset_val_ew = dataset_val[cols_ew]\n",
    "dataset_val_bio = dataset_val[cols_bio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961a268a-e0eb-4f9d-a0e3-d3dcec7a876b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make data into tensor objects\n",
    "X_train_rf_tensor = torch.tensor(X_train_rf.values, dtype=torch.float32)\n",
    "X_train_ew_tensor = torch.tensor(X_train_ew.values, dtype=torch.float32)\n",
    "X_train_bio_tensor = torch.tensor(X_train_bio.values, dtype=torch.float32)\n",
    "\n",
    "X_test_rf_tensor = torch.tensor(X_test_rf.values, dtype=torch.float32)\n",
    "X_test_ew_tensor = torch.tensor(X_test_ew.values, dtype=torch.float32)\n",
    "X_test_bio_tensor = torch.tensor(X_test_bio.values, dtype=torch.float32)\n",
    "\n",
    "y_train_rf_tensor = torch.tensor(y_train_rf['Reforest'].values, dtype=torch.float32)\n",
    "y_train_ew_tensor = torch.tensor(y_train_ew['EW'].values, dtype=torch.float32)\n",
    "y_train_bio_tensor = torch.tensor(y_train_bio['Biochar'].values, dtype=torch.float32)\n",
    "\n",
    "y_test_rf_tensor = torch.tensor(y_test_rf['Reforest'].values, dtype=torch.float32)\n",
    "y_test_ew_tensor = torch.tensor(y_test_ew['EW'].values, dtype=torch.float32)\n",
    "y_test_bio_tensor = torch.tensor(y_test_bio['Biochar'].values, dtype=torch.float32)\n",
    "\n",
    "# Make test data into tensor objects\n",
    "X_val_rf_tensor = torch.tensor(dataset_val_rf.values, dtype=torch.float32)\n",
    "X_val_ew_tensor = torch.tensor(dataset_val_ew.values, dtype=torch.float32)\n",
    "X_val_bio_tensor = torch.tensor(dataset_val_bio.values, dtype=torch.float32)\n",
    "\n",
    "#train data\n",
    "training_data_rf = TensorDataset(X_train_rf_tensor, y_train_rf_tensor)\n",
    "train_dataloader_rf = DataLoader(training_data_rf, batch_size=64)\n",
    "\n",
    "training_data_ew = TensorDataset(X_train_ew_tensor, y_train_ew_tensor)\n",
    "train_dataloader_ew = DataLoader(training_data_ew, batch_size=64)\n",
    "\n",
    "training_data_bio = TensorDataset(X_train_bio_tensor, y_train_bio_tensor)\n",
    "train_dataloader_bio = DataLoader(training_data_bio, batch_size=64)\n",
    "\n",
    "\n",
    "#test data\n",
    "test_data_rf = TensorDataset(X_test_rf_tensor,  y_test_rf_tensor)\n",
    "test_dataloader_rf = DataLoader(test_data_rf, batch_size=64)\n",
    "\n",
    "test_data_ew = TensorDataset(X_test_ew_tensor,  y_test_ew_tensor)\n",
    "test_dataloader_ew = DataLoader(test_data_ew, batch_size=64)\n",
    "\n",
    "test_data_bio = TensorDataset(X_test_bio_tensor,  y_test_bio_tensor)\n",
    "test_dataloader_bio = DataLoader(test_data_bio, batch_size=64)\n",
    "\n",
    "\n",
    "#validate data\n",
    "val_data_rf = TensorDataset(X_val_rf_tensor)\n",
    "val_dataloader_rf = DataLoader(val_data_rf, batch_size=64)\n",
    "\n",
    "val_data_ew = TensorDataset(X_val_ew_tensor)\n",
    "val_dataloader_ew = DataLoader(val_data_ew, batch_size=64)\n",
    "\n",
    "val_data_bio = TensorDataset(X_val_bio_tensor)\n",
    "val_dataloader_bio = DataLoader(val_data_bio, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5417b-801b-4e95-95f6-3bc7e435b74f",
   "metadata": {},
   "source": [
    "# Supervised Neural Net for suitability prediction\n",
    "## DeepTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92752922-a724-4551-be62-49d394efe610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "LEARNING_RATE = 0.0001\n",
    "DNN_PARAMS = {'hidden_units': ((128, 0, False), (64, 0, False)), 'dnn_activation': 'relu'}\n",
    "EPOCHS = 20\n",
    "ESP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9547d72-8738-4d2a-9ad1-3d2cfb5edf18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:24:00 I deeptables.m.deeptable.py 337 - X.Shape=torch.Size([67, 11]), y.Shape=(67,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dcn_nets'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.optimizers.optimizer_experimental.rmsprop.RMSprop object at 0x7f4917b6f450>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-17 16:24:00 I deeptables.m.deeptable.py 338 - metrics:['accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:24:00 W hypernets.t.cache.py 210 - TypeError: can't pickle weakref objects\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:24:00 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:24:00 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:24:00 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-17 16:24:00 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.007425069808959961s\n",
      "04-17 16:24:00 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-17 16:24:00 I deeptables.m.preprocessor.py 383 - Imputation taken 0.013078689575195312s\n",
      "04-17 16:24:00 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-17 16:24:00 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.11405777931213379s\n",
      "04-17 16:24:00 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.15387868881225586s\n",
      "04-17 16:24:00 I deeptables.m.deeptable.py 353 - Training...\n",
      "04-17 16:24:00 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_accuracy, patience:5, mode:max\n",
      "04-17 16:24:00 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-17 16:24:00 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-17 16:24:00 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:24:00 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (11)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [69, 69, 69, 68, 69, 69, 69, 69, 69, 69, 69]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 44)\n",
      "---------------------------------------------------------\n",
      "nets: ['dcn_nets']\n",
      "---------------------------------------------------------\n",
      "dcn-widecross: input_shape (None, 44), output_shape (None, 44)\n",
      "dcn-dnn2: input_shape (None, 44), output_shape (None, 64)\n",
      "dcn: input_shape (None, 44), output_shape (None, 108)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: <keras.optimizers.optimizer_experimental.rmsprop.RMSprop object at 0x7f4917b6f450>\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-17 16:24:00 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5702 - accuracy: 0.3585 - val_loss: 1.3982 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.9470 - accuracy: 0.3396 - val_loss: 1.3971 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3414 - accuracy: 0.3396 - val_loss: 1.3961 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6986 - accuracy: 0.2453 - val_loss: 1.3947 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5624 - accuracy: 0.3396 - val_loss: 1.3935 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6282 - accuracy: 0.3774Restoring model weights from the end of the best epoch: 1.\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6282 - accuracy: 0.3774 - val_loss: 1.3928 - val_accuracy: 0.0000e+00\n",
      "Epoch 6: early stopping\n",
      "04-17 16:24:03 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-17 16:24:03 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "04-17 16:24:03 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240417162400_dcn_nets/dcn_nets.h5\n",
      "04-17 16:24:03 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:24:03 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:24:03 I deeptables.m.preprocessor.py 249 - transform_X taken 0.038516998291015625s\n",
      "04-17 16:24:03 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-17 16:24:03 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0002837181091308594s\n",
      "04-17 16:24:03 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-17 16:24:03 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "04-17 16:24:03 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-17 16:24:03 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:24:03 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:24:03 I deeptables.m.preprocessor.py 249 - transform_X taken 0.03861689567565918s\n",
      "04-17 16:24:03 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-17 16:24:03 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-17 16:24:03 I deeptables.m.deeptable.py 559 - predict_proba taken 0.30714917182922363s\n",
      "04-17 16:24:03 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.3612148761749268, 'accuracy': 0.4117647111415863}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = deeptable.ModelConfig(\n",
    "    dnn_params=DNN_PARAMS,\n",
    "    nets=['dcn_nets'],\n",
    "    optimizer=RMSprop(learning_rate=LEARNING_RATE),\n",
    "    earlystopping_patience=ESP)\n",
    "dt_rf = deeptable.DeepTable(config=conf)\n",
    "dt_rf.fit(X_train_rf_tensor, y_train_rf['Reforest'], epochs=EPOCHS)\n",
    "score_rf = dt_rf.evaluate(X_test_rf_tensor, y_test_rf['Reforest'])\n",
    "preds_rf = dt_rf.predict(X_test_rf_tensor)\n",
    "score_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d546d27-f6f5-4c99-b436-7c293a110180",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:47 I deeptables.m.deeptable.py 337 - X.Shape=torch.Size([67, 12]), y.Shape=(67,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dcn_nets'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.optimizers.optimizer_experimental.rmsprop.RMSprop object at 0x7f4932cdc5d0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-17 16:23:47 I deeptables.m.deeptable.py 338 - metrics:['accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:47 W hypernets.t.cache.py 210 - TypeError: can't pickle weakref objects\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:47 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:47 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10', 'x_11'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:47 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-17 16:23:47 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.008162260055541992s\n",
      "04-17 16:23:47 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-17 16:23:47 I deeptables.m.preprocessor.py 383 - Imputation taken 0.01774454116821289s\n",
      "04-17 16:23:47 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-17 16:23:47 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.11747145652770996s\n",
      "04-17 16:23:47 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.15785646438598633s\n",
      "04-17 16:23:47 I deeptables.m.deeptable.py 353 - Training...\n",
      "04-17 16:23:47 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_accuracy, patience:5, mode:max\n",
      "04-17 16:23:47 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-17 16:23:47 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-17 16:23:47 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:47 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (12)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [69, 69, 69, 69, 69, 69, 69, 69, 69, 40, 52, 69]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 48)\n",
      "---------------------------------------------------------\n",
      "nets: ['dcn_nets']\n",
      "---------------------------------------------------------\n",
      "dcn-widecross: input_shape (None, 48), output_shape (None, 48)\n",
      "dcn-dnn2: input_shape (None, 48), output_shape (None, 64)\n",
      "dcn: input_shape (None, 48), output_shape (None, 112)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: <keras.optimizers.optimizer_experimental.rmsprop.RMSprop object at 0x7f4932cdc5d0>\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-17 16:23:47 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.8895 - accuracy: 0.3208 - val_loss: 1.3938 - val_accuracy: 0.1429\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2303 - accuracy: 0.3396 - val_loss: 1.3940 - val_accuracy: 0.1429\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9849 - accuracy: 0.4151 - val_loss: 1.3942 - val_accuracy: 0.1429\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3724 - accuracy: 0.3208 - val_loss: 1.3945 - val_accuracy: 0.1429\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5512 - accuracy: 0.3962 - val_loss: 1.3947 - val_accuracy: 0.1429\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1017 - accuracy: 0.4151Restoring model weights from the end of the best epoch: 1.\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.1017 - accuracy: 0.4151 - val_loss: 1.3949 - val_accuracy: 0.1429\n",
      "Epoch 6: early stopping\n",
      "04-17 16:23:50 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-17 16:23:50 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "04-17 16:23:50 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240417162347_dcn_nets/dcn_nets.h5\n",
      "04-17 16:23:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:50 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10', 'x_11'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.03966379165649414s\n",
      "04-17 16:23:50 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-17 16:23:50 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0002162456512451172s\n",
      "04-17 16:23:50 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-17 16:23:50 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "04-17 16:23:50 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-17 16:23:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:50 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10', 'x_11'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.04038500785827637s\n",
      "04-17 16:23:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-17 16:23:50 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-17 16:23:51 I deeptables.m.deeptable.py 559 - predict_proba taken 0.2461109161376953s\n",
      "04-17 16:23:51 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.3759139776229858, 'accuracy': 0.47058823704719543}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = deeptable.ModelConfig(\n",
    "    dnn_params=DNN_PARAMS,\n",
    "    nets=['dcn_nets'],\n",
    "    optimizer=RMSprop(learning_rate=LEARNING_RATE),\n",
    "    earlystopping_patience=ESP)\n",
    "dt_ew = deeptable.DeepTable(config=conf)\n",
    "dt_ew.fit(X_train_ew_tensor, y_train_ew['EW'], epochs=EPOCHS)\n",
    "score_ew = dt_ew.evaluate(X_test_ew_tensor, y_test_ew['EW'])\n",
    "preds_ew = dt_ew.predict(X_test_ew_tensor)\n",
    "score_ew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6dcb84fe-5709-41fc-b850-d3e2c947a10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:14 I deeptables.m.deeptable.py 337 - X.Shape=torch.Size([67, 12]), y.Shape=(67,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dcn_nets'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.optimizers.optimizer_experimental.rmsprop.RMSprop object at 0x7f493331d690>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-17 16:23:14 I deeptables.m.deeptable.py 338 - metrics:['accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:14 W hypernets.t.cache.py 210 - TypeError: can't pickle weakref objects\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:14 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:14 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10', 'x_11'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:14 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-17 16:23:14 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.00841379165649414s\n",
      "04-17 16:23:14 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-17 16:23:14 I deeptables.m.preprocessor.py 383 - Imputation taken 0.015813350677490234s\n",
      "04-17 16:23:14 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-17 16:23:15 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.16371631622314453s\n",
      "04-17 16:23:15 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.22676992416381836s\n",
      "04-17 16:23:15 I deeptables.m.deeptable.py 353 - Training...\n",
      "04-17 16:23:15 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_accuracy, patience:5, mode:max\n",
      "04-17 16:23:15 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-17 16:23:15 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-17 16:23:15 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-17 16:23:15 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (12)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 48)\n",
      "---------------------------------------------------------\n",
      "nets: ['dcn_nets']\n",
      "---------------------------------------------------------\n",
      "dcn-widecross: input_shape (None, 48), output_shape (None, 48)\n",
      "dcn-dnn2: input_shape (None, 48), output_shape (None, 64)\n",
      "dcn: input_shape (None, 48), output_shape (None, 112)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: <keras.optimizers.optimizer_experimental.rmsprop.RMSprop object at 0x7f493331d690>\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-17 16:23:15 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrn4ch/.conda/envs/myenv/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 1.9985 - accuracy: 0.2830 - val_loss: 1.3776 - val_accuracy: 0.2143\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.0386 - accuracy: 0.2264 - val_loss: 1.3775 - val_accuracy: 0.2143\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.0544 - accuracy: 0.3019 - val_loss: 1.3768 - val_accuracy: 0.2857\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7842 - accuracy: 0.2642 - val_loss: 1.3772 - val_accuracy: 0.2857\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.7817 - accuracy: 0.4151 - val_loss: 1.3776 - val_accuracy: 0.2857\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.7985 - accuracy: 0.3396 - val_loss: 1.3774 - val_accuracy: 0.2857\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8776 - accuracy: 0.2830 - val_loss: 1.3770 - val_accuracy: 0.2857\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0011 - accuracy: 0.3019Restoring model weights from the end of the best epoch: 3.\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0011 - accuracy: 0.3019 - val_loss: 1.3765 - val_accuracy: 0.2857\n",
      "Epoch 8: early stopping\n",
      "04-17 16:23:18 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-17 16:23:18 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "04-17 16:23:18 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240417162314_dcn_nets/dcn_nets.h5\n",
      "04-17 16:23:18 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:18 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10', 'x_11'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.040585994720458984s\n",
      "04-17 16:23:18 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-17 16:23:18 I deeptables.m.preprocessor.py 236 - transform_y taken 0.00028204917907714844s\n",
      "04-17 16:23:18 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-17 16:23:18 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "04-17 16:23:18 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-17 16:23:18 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-17 16:23:18 W deeptables.m.preprocessor.py 154 - Column index of X has been converted: Index(['x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9',\n",
      "       'x_10', 'x_11'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-17 16:23:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.041500091552734375s\n",
      "04-17 16:23:18 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-17 16:23:18 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-17 16:23:18 I deeptables.m.deeptable.py 559 - predict_proba taken 0.24778962135314941s\n",
      "04-17 16:23:18 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.3834924697875977, 'accuracy': 0.4117647111415863}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = deeptable.ModelConfig(\n",
    "    dnn_params=DNN_PARAMS,\n",
    "    nets=['dcn_nets'],\n",
    "    optimizer=RMSprop(learning_rate=LEARNING_RATE),\n",
    "    earlystopping_patience=ESP)\n",
    "dt_bio = deeptable.DeepTable(config=conf)\n",
    "dt_bio.fit(X_train_bio_tensor, y_train_bio['Biochar'], epochs=EPOCHS)\n",
    "score_bio = dt_bio.evaluate(X_test_bio_tensor, y_test_bio['Biochar'])\n",
    "preds_bio = dt_bio.predict(X_test_bio_tensor)\n",
    "score_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "163546c1-ee77-494c-83e4-3498b3205b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf: {'loss': 1.3612148761749268, 'accuracy': 0.4117647111415863} [1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1]\n",
      "ew: {'loss': 1.3759139776229858, 'accuracy': 0.47058823704719543} [1 1 1 3 1 1 1 1 1 1 3 3 3 1 3 1 1]\n",
      "bio: {'loss': 1.3834924697875977, 'accuracy': 0.4117647111415863} [3 1 3 3 3 3 1 3 3 3 1 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print('rf:', score_rf, preds_rf)\n",
    "print('ew:', score_ew, preds_ew)\n",
    "print('bio:', score_bio, preds_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "141fd0fb-200d-497f-be7c-bfb598197800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy is 0.4313725531101227\n"
     ]
    }
   ],
   "source": [
    "print('average accuracy is',(score_rf['accuracy']+score_ew['accuracy']+score_bio['accuracy'])/3) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
